{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10776101,"sourceType":"datasetVersion","datasetId":6685957}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains the bulk of my 3rd place solution. This notebook uses some basic default XGBoost parameters, but for my final solution I used an average of 17 different tuned hyperparameters (improvement of ~.55 WMAE). For more details on the rest of my solution, check out my discussion post at https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/discussion/563064","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RepeatedKFold\nfrom xgboost import XGBRegressor, DMatrix\nimport shap\nimport lightgbm as lgb\nshap.initjs()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:10:02.867523Z","iopub.execute_input":"2025-02-17T18:10:02.867875Z","iopub.status.idle":"2025-02-17T18:10:02.878629Z","shell.execute_reply.started":"2025-02-17T18:10:02.867847Z","shell.execute_reply":"2025-02-17T18:10:02.877786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv').drop(['warehouse','product_unique_id'],axis=1)\ninventory.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:10:03.401572Z","iopub.execute_input":"2025-02-17T18:10:03.401832Z","iopub.status.idle":"2025-02-17T18:10:03.443833Z","shell.execute_reply.started":"2025-02-17T18:10:03.40181Z","shell.execute_reply":"2025-02-17T18:10:03.443169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One thing I found very fruitful was extracting the word part of the 'name' field (in my code this I call this the 'common_name' and using it as a grouping column for various aggregations of other features.","metadata":{}},{"cell_type":"markdown","source":"Generally speaking, I didn't have a lot of success trying to extract additional features from the calendar. All I ended up grabbing were binary indicators of holidays, the day before a holiday, and the day after a holiday.","metadata":{}},{"cell_type":"code","source":"def fe_date(df):\n    df['year'] = df['date'].dt.year\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['days_since_2020'] = (df['date'] - pd.to_datetime('2020-01-01')).dt.days.astype('int')\n    df['day_of_year'] = df['date'].dt.dayofyear\n    df['cos_day'] = np.cos(df['day_of_year']*2*np.pi/365)\n    df['sin_day'] = np.sin(df['day_of_year']*2*np.pi/365)\n\ndef fe_other(df):\n    discount_cols = ['type_0_discount','type_1_discount','type_2_discount','type_3_discount','type_4_discount','type_5_discount','type_6_discount']\n    df[discount_cols] = df[discount_cols].clip(0)\n    df['max_discount'] = df[['type_0_discount','type_1_discount','type_2_discount','type_3_discount','type_4_discount','type_5_discount']].max(axis=1)\n    \n    # Given that we're using XGBoost, which is in theory invariant to monotonic transformations of features, this transformation in isolation doesn't really do anything. I mainly did it because it made the shap plot look more linear. However, I think it did make further feature engineering that used price more effective.\n    df['sell_price_main'] = np.log(df['sell_price_main']) \n\n    df['common_name'] = df['name'].apply(lambda x: x[:x.find('_')])\n    df['CN_total_products'] = df.groupby(['date','warehouse','common_name'])['unique_id'].transform('nunique')\n    df['CN_discount_avg'] = df.groupby(['date','warehouse','common_name'])['max_discount'].transform('mean')\n    df['CN_WH'] = df['common_name'] + '_' + df['warehouse']\n    df['name_num_warehouses'] = df.groupby(['date','name'])['unique_id'].transform('nunique')\n\ndef fe_combined(df):\n    df['num_sales_days_28D'] = pd.MultiIndex.from_frame(df[['unique_id','date']]).map(df.sort_values('date').groupby('unique_id').rolling(\n        window='28D', on='date', closed='left')['date'].count().fillna(0))\n\n    # This 'price_detrended' feature was one I found pretty late into the game, but I think it helped out a lot. I was trying to make a feature that captured whether an item was cheap or expensive relative to its usual price, which is what 'price_scaled' represents. What I found was that the prices of things generally increase over time. So I removed that time-based trend to construct price_detrended, and that proved very effective.\n    mean_prices = df.groupby(df['unique_id'])['sell_price_main'].mean()\n    std_prices = df.groupby(df['unique_id'])['sell_price_main'].std()\n    df['price_scaled'] = np.where(df['unique_id'].map(std_prices) == 0, 0, \n                                  (df['sell_price_main'] - df['unique_id'].map(mean_prices))/df['unique_id'].map(std_prices))\n    df['price_detrended'] = df['price_scaled'] - df.groupby(['days_since_2020','warehouse'])['price_scaled'].transform('mean')\n    df.drop('price_scaled',axis=1,inplace=True)\n\n    warehouse_stats = df.groupby(['date','warehouse'])['total_orders'].median().rename('med_total_orders').reset_index().sort_values('date')\n    warehouse_stats['ewmean_orders_56'] = warehouse_stats.groupby('warehouse')['med_total_orders'].transform(lambda x:x.ewm(alpha=1/56).mean())\n    df['mean_orders_14d'] = pd.MultiIndex.from_frame(df[['warehouse','date']]).map(\n        warehouse_stats.groupby('warehouse').rolling(on='date',window='14D')['med_total_orders'].mean())\n    df['ewmean_orders_56'] = pd.MultiIndex.from_frame(df[['warehouse','date']]).map(\n        warehouse_stats.set_index(['warehouse','date'])['ewmean_orders_56'])\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:10:07.587616Z","iopub.execute_input":"2025-02-17T18:10:07.587932Z","iopub.status.idle":"2025-02-17T18:10:07.59822Z","shell.execute_reply.started":"2025-02-17T18:10:07.587906Z","shell.execute_reply":"2025-02-17T18:10:07.597354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar.loc[calendar['holiday_name'].isna(), 'holiday'] = 0 # V3\ncalendar['last_holiday_date'] = calendar['date']\ncalendar['next_holiday_date'] = calendar['date']\ncalendar.loc[calendar['holiday'] == 0, ['last_holiday_date','next_holiday_date']] = np.nan\ncalendar['last_holiday_date'] = calendar.sort_values('date').groupby('warehouse')['last_holiday_date'].ffill()\ncalendar['next_holiday_date'] = calendar.sort_values('date').groupby('warehouse')['next_holiday_date'].bfill()\ncalendar['days_since_last_holiday'] = ((calendar['date'] - calendar['last_holiday_date']).dt.days)\ncalendar['days_to_next_holiday'] = ((calendar['next_holiday_date'] - calendar['date']).dt.days)\ncalendar['day_before_holiday'] = calendar['days_to_next_holiday'] == 1\ncalendar['day_after_holiday'] = calendar['days_since_last_holiday'] == 1\ncalendar.drop(['last_holiday_date','next_holiday_date'],axis=1,inplace=True)\ncalendar.drop(['days_since_last_holiday','days_to_next_holiday'],axis=1,inplace=True)\ncalendar.drop(['shops_closed','winter_school_holidays','school_holidays','holiday_name'],axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:10:09.08673Z","iopub.execute_input":"2025-02-17T18:10:09.08701Z","iopub.status.idle":"2025-02-17T18:10:09.152831Z","shell.execute_reply.started":"2025-02-17T18:10:09.08699Z","shell.execute_reply":"2025-02-17T18:10:09.15187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\ntrain['id'] = train['unique_id'].astype('str') + '_' + train['date'].astype('str')\ntrain.set_index('id',inplace=True)\ntrain = train[~train['sales'].isna()]\ntrain = train.reset_index().merge(inventory, on='unique_id').set_index('id').loc[train.index]\ntrain = train.reset_index().merge(calendar, on=['date','warehouse']).set_index('id').loc[train.index]\nfe_date(train)\nfe_other(train)\n\nall_data = train\nall_data = fe_combined(all_data)\n\ntrain = all_data[all_data['date'] < pd.to_datetime('2024-05-20')]\ntest = all_data[all_data['date'] >= pd.to_datetime('2024-05-20')]\ntest_sales = test['sales']\ntest = test.drop(columns=['sales', 'availability'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:10:10.630097Z","iopub.execute_input":"2025-02-17T18:10:10.630444Z","iopub.status.idle":"2025-02-17T18:11:20.634086Z","shell.execute_reply.started":"2025-02-17T18:10:10.630414Z","shell.execute_reply":"2025-02-17T18:11:20.633416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train.drop('sales',axis=1)\ny_train = train['sales']\ntrain_availability = X_train['availability']\nX_train.drop('availability',inplace=True,axis=1)\nweights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv').set_index('unique_id')\nX_train_weights = X_train['unique_id'].map(weights['weight'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:11:20.63508Z","iopub.execute_input":"2025-02-17T18:11:20.635293Z","iopub.status.idle":"2025-02-17T18:11:22.331378Z","shell.execute_reply.started":"2025-02-17T18:11:20.635275Z","shell.execute_reply":"2025-02-17T18:11:22.330715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols = ['unique_id'] + list(X_train.columns[X_train.dtypes == 'object'])\nall_data = pd.concat([X_train, test])\nadd_cols = ['last_sales_ema005','CN_sales_sum','last_sales_zs']\n\n# Here there are a few additional features engineered from historical sales data. These are done separately from the rest of my feature engineering because when I go to test model performance on a time-based holdout validation set, I need to make sure these features aren't using sales data from that validation set.\ntrain_cp = train.groupby('unique_id')['date'].apply(lambda s: pd.date_range(s.min(), test.date.max())).explode().reset_index()\ntrain_cp = train_cp.merge(\n    pd.concat([train[['unique_id','date','sales','warehouse',]], \n               test[['unique_id','date','warehouse']]]),\n    on=['unique_id','date'],how='left')\ntrain_cp = train_cp.merge(inventory, left_on='unique_id', right_index=True)\ntrain_cp['common_name'] = train_cp['name'].apply(lambda x: x[:x.find('_')])\ntrain_cp.sort_values('date',inplace=True)\ntrain_cp['last_sales_ema005'] = train_cp.groupby(['unique_id'])['sales'].transform(lambda x: x.shift(1).ewm(alpha=.005).mean()).fillna(0)\ntrain_cp['CN_sales_sum'] = train_cp.groupby(['common_name','warehouse','date'])['last_sales_ema005'].transform('sum')\nall_data = all_data.merge(train_cp.set_index(['unique_id','date'])[[\n    'last_sales_ema005','CN_sales_sum'\n]], left_on=['unique_id','date'],right_index=True,how='left')\nsales_stats = train_cp.groupby(['common_name','warehouse'])['sales'].agg(['mean','std'])\nall_data['last_sales_zs'] = (all_data['last_sales_ema005'] - pd.MultiIndex.from_frame(all_data[['common_name','warehouse']]).map(\n    sales_stats['mean']))/ pd.MultiIndex.from_frame(all_data[['common_name','warehouse']]).map(sales_stats['std'])\n\n# Cutting all data prior to 2022 seems to help. This could be due to COVID effects, and also the fact that there is little data from the Germany warehouses before 2022.\nX_train = X_train[X_train['date'] >= '2022-01-01']\ny_train = y_train.loc[X_train.index]\nX_train_weights = X_train_weights.loc[X_train.index]\n\nX_train[add_cols] = all_data[add_cols]\ntest[add_cols] = all_data[add_cols]\nall_data[cat_cols] = all_data[cat_cols].astype('str').astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:11:22.332523Z","iopub.execute_input":"2025-02-17T18:11:22.332725Z","iopub.status.idle":"2025-02-17T18:12:28.185669Z","shell.execute_reply.started":"2025-02-17T18:11:22.332706Z","shell.execute_reply":"2025-02-17T18:12:28.184946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save all data for experiments\n# import os\n# SAVE_DIRECTORY = '/kaggle/working/dfs/'\n# os.makedirs(SAVE_DIRECTORY, exist_ok=True)\n\n# X_train.to_csv(SAVE_DIRECTORY + 'X_train.csv', index=False)\n# y_train.to_csv(SAVE_DIRECTORY + 'y_train.csv', index=False)\n# test.to_csv(SAVE_DIRECTORY + 'test.csv', index=False)\n# test_sales.to_csv(SAVE_DIRECTORY + 'test_sales.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:26:08.730265Z","iopub.execute_input":"2025-02-17T16:26:08.730569Z","iopub.status.idle":"2025-02-17T16:27:15.574143Z","shell.execute_reply.started":"2025-02-17T16:26:08.730543Z","shell.execute_reply":"2025-02-17T16:27:15.573148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trying own simple model - LGBM","metadata":{}},{"cell_type":"code","source":"# DF_DIRECTORY = '/kaggle/input/3rd-place-dfs/'\n# X_train = pd.read_csv(DF_DIRECTORY + 'X_train.csv')\n# y_train = pd.read_csv(DF_DIRECTORY + 'y_train.csv')\n# test = pd.read_csv(DF_DIRECTORY + 'test.csv')\n# test_sales = pd.read_csv(DF_DIRECTORY + 'test_sales.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:01:58.026788Z","iopub.execute_input":"2025-02-17T18:01:58.02712Z","iopub.status.idle":"2025-02-17T18:02:21.583662Z","shell.execute_reply.started":"2025-02-17T18:01:58.027095Z","shell.execute_reply":"2025-02-17T18:02:21.582717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.setdiff1d(weights['unique_id'], X_train['unique_id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:24:36.116857Z","iopub.execute_input":"2025-02-17T18:24:36.117136Z","iopub.status.idle":"2025-02-17T18:24:36.159241Z","shell.execute_reply.started":"2025-02-17T18:24:36.117116Z","shell.execute_reply":"2025-02-17T18:24:36.158432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')\\\n.set_index('unique_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:25:49.739073Z","iopub.execute_input":"2025-02-17T18:25:49.739404Z","iopub.status.idle":"2025-02-17T18:25:49.749471Z","shell.execute_reply.started":"2025-02-17T18:25:49.739376Z","shell.execute_reply":"2025-02-17T18:25:49.748581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_weight = X_train['unique_id'].map(weights['weight'])\ntest_weight = test['unique_id'].map(weights['weight'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:25:52.374156Z","iopub.execute_input":"2025-02-17T18:25:52.374489Z","iopub.status.idle":"2025-02-17T18:25:52.398557Z","shell.execute_reply.started":"2025-02-17T18:25:52.374463Z","shell.execute_reply":"2025-02-17T18:25:52.397908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"object_cols = X_train.select_dtypes(include=['object']).columns\nX_train[object_cols] = X_train[object_cols].astype('category')\ntest[object_cols] = test[object_cols].astype('category')\n\nspecial = 'CN_WH'\nX_train[special], _ = X_train[special].factorize()\ntest[special], _ = test[special].factorize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:25:54.584766Z","iopub.execute_input":"2025-02-17T18:25:54.585061Z","iopub.status.idle":"2025-02-17T18:25:54.612671Z","shell.execute_reply.started":"2025-02-17T18:25:54.585027Z","shell.execute_reply":"2025-02-17T18:25:54.612013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_weighted_squared_mae(preds, dtrain):\n    # Retrieve true labels\n    labels = dtrain.get_label()\n    \n    # Retrieve weights; if not provided, default to an array of ones\n    weights = dtrain.get_weight()\n    if weights is None or len(weights) == 0:\n        weights = np.ones_like(labels)\n    \n    # Square both predictions and labels\n    squared_preds = preds ** 2\n    squared_labels = labels ** 2\n    \n    # Compute the weighted mean absolute error\n    weighted_mae = np.sum(np.abs(squared_preds - squared_labels) * weights) / np.sum(weights)\n    \n    # Return a tuple: (metric_name, metric_value, is_higher_better)\n    return 'weighted_squared_mae', weighted_mae, False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:25:56.196634Z","iopub.execute_input":"2025-02-17T18:25:56.196954Z","iopub.status.idle":"2025-02-17T18:25:56.201912Z","shell.execute_reply.started":"2025-02-17T18:25:56.196927Z","shell.execute_reply":"2025-02-17T18:25:56.201027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Define features (exclude target and date; adjust if you need to drop additional columns)\nfeatures = [col for col in X_train.columns if col not in ['date', 'name']]\n\n# Create LightGBM Datasets\nlgb_train = lgb.Dataset(X_train[features], label=np.sqrt(y_train), weight=train_weight)\nlgb_valid = lgb.Dataset(test[features], label=np.sqrt(test_sales), reference=lgb_train, weight=test_weight)\n\n# Define LightGBM parameters (you can adjust these)\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'seed': 40,\n    'learning_rate': 0.1,\n    'verbosity': -1,\n    'device': 'gpu', \n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'max_bins': 255\n}\n\n# Train the model with early stopping\nmodel = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=5000,\n    valid_sets=[lgb_train, lgb_valid],\n    valid_names=['train', 'valid'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100),\n        lgb.log_evaluation(period=100)\n    ],\n    feval=custom_weighted_squared_mae\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:25:57.466111Z","iopub.execute_input":"2025-02-17T18:25:57.466458Z","iopub.status.idle":"2025-02-17T18:27:13.26366Z","shell.execute_reply.started":"2025-02-17T18:25:57.466429Z","shell.execute_reply":"2025-02-17T18:27:13.262779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntrain_preds = model.predict(X_train[features])\ntest_preds = model.predict(test[features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:16:28.716163Z","iopub.execute_input":"2025-02-17T18:16:28.716524Z","iopub.status.idle":"2025-02-17T18:19:03.52174Z","shell.execute_reply.started":"2025-02-17T18:16:28.716493Z","shell.execute_reply":"2025-02-17T18:19:03.520878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_mae = mean_absolute_error(train_preds**2, y_train, sample_weight = train_weight)\ntest_mae = mean_absolute_error(test_preds**2, test_sales, sample_weight = test_weight)\nprint(train_mae)\nprint(test_mae)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:19:48.713685Z","iopub.execute_input":"2025-02-17T18:19:48.714001Z","iopub.status.idle":"2025-02-17T18:19:48.740536Z","shell.execute_reply.started":"2025-02-17T18:19:48.713974Z","shell.execute_reply":"2025-02-17T18:19:48.739667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.Series(train_weight).isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:22:35.514457Z","iopub.execute_input":"2025-02-17T18:22:35.5148Z","iopub.status.idle":"2025-02-17T18:22:35.529806Z","shell.execute_reply.started":"2025-02-17T18:22:35.514773Z","shell.execute_reply":"2025-02-17T18:22:35.529014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb.plot_importance(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:25:12.163669Z","iopub.execute_input":"2025-02-17T05:25:12.16401Z","iopub.status.idle":"2025-02-17T05:25:12.738117Z","shell.execute_reply.started":"2025-02-17T05:25:12.163986Z","shell.execute_reply":"2025-02-17T05:25:12.737198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\n\nX_subset = X_train[features].sample(10000)\nshap_values = explainer.shap_values(X_subset)\n\nshap.summary_plot(shap_values, X_subset, plot_type=\"dot\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:33:34.59848Z","iopub.execute_input":"2025-02-17T05:33:34.598817Z","iopub.status.idle":"2025-02-17T05:35:14.932118Z","shell.execute_reply.started":"2025-02-17T05:33:34.598747Z","shell.execute_reply":"2025-02-17T05:35:14.930997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shap.dependence_plot('total_orders', shap_values, X_subset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:40:30.692185Z","iopub.execute_input":"2025-02-17T05:40:30.692505Z","iopub.status.idle":"2025-02-17T05:40:31.71357Z","shell.execute_reply.started":"2025-02-17T05:40:30.692477Z","shell.execute_reply":"2025-02-17T05:40:31.712674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGboost model","metadata":{}},{"cell_type":"code","source":"def custom_weighted_squared_mae(preds, dtrain):\n    # Retrieve true labels\n    labels = dtrain.get_label()\n    \n    # Retrieve weights; if not provided, default to an array of ones\n    weights = dtrain.get_weight()\n    if weights is None or len(weights) == 0:\n        weights = np.ones_like(labels)\n    \n    # Square both predictions and labels\n    squared_preds = preds ** 2\n    squared_labels = labels ** 2\n    \n    # Compute the weighted mean absolute error\n    weighted_mae = np.sum(np.abs(squared_preds - squared_labels) * weights) / np.sum(weights)\n    \n    # Return a tuple: (metric_name, metric_value, is_higher_better)\n    return ('mae', weighted_mae)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:49:40.912366Z","iopub.execute_input":"2025-02-17T16:49:40.912703Z","iopub.status.idle":"2025-02-17T16:49:40.91749Z","shell.execute_reply.started":"2025-02-17T16:49:40.912676Z","shell.execute_reply":"2025-02-17T16:49:40.916546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\n# Define features (exclude target and date; adjust if you need to drop additional columns)\nfeatures = [col for col in X_train.columns if col not in ['date', 'name']]\n\nlr = .1\nes = 100\nn_est = round(5000/lr)\nseed = 2\nbase_params = {\n    'n_estimators':n_est\n    ,'learning_rate':lr\n    ,'verbosity':0\n    ,'enable_categorical':True\n    ,'early_stopping_rounds':es\n    ,'random_state':seed\n    ,'objective':'reg:squarederror'\n    ,'eval_metric':'rmse'\n    ,'device':'cuda'\n    ,'reg_lambda':0\n    ,'min_child_weight':1\n}\n\nxgb = XGBRegressor(**base_params)\nxgb.fit(X_train[features],\n        np.sqrt(y_train),\n        sample_weight=train_weight,\n        eval_set=[(test[features], np.sqrt(test_sales))],\n        eval_sample_weight=[test_weight],\n        verbose=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:26:28.941021Z","iopub.execute_input":"2025-02-17T17:26:28.941306Z","iopub.status.idle":"2025-02-17T17:26:29.212946Z","shell.execute_reply.started":"2025-02-17T17:26:28.941285Z","shell.execute_reply":"2025-02-17T17:26:29.212213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntrain_preds = xgb.predict(X_train[features])\ntest_preds = xgb.predict(test[features])\n\ntrain_mae = mean_absolute_error(train_preds**2, y_train, sample_weight = train_weight)\ntest_mae = mean_absolute_error(test_preds**2, test_sales, sample_weight = test_weight)\nprint(train_mae)\nprint(test_mae)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:20:37.108546Z","iopub.execute_input":"2025-02-17T17:20:37.108904Z","iopub.status.idle":"2025-02-17T17:20:47.638415Z","shell.execute_reply.started":"2025-02-17T17:20:37.108874Z","shell.execute_reply":"2025-02-17T17:20:47.637634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = xgb.plot_importance(model, max_num_features=10, importance_type='gain')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:22:21.81815Z","iopub.execute_input":"2025-02-17T17:22:21.81843Z","iopub.status.idle":"2025-02-17T17:22:22.120049Z","shell.execute_reply.started":"2025-02-17T17:22:21.818411Z","shell.execute_reply":"2025-02-17T17:22:22.118937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Per a suggestion from https://datascience.stackexchange.com/questions/104743/optimizing-mae-degrades-mae-metrics, I take the square root of sales and optimize the mean squared error as a sort-of proxy for optimizing mean absolute error. In this case, it's more effective than directly trying to optimize for mean absolute error.","metadata":{}},{"cell_type":"markdown","source":"## Looking at model outputs","metadata":{}},{"cell_type":"code","source":"concatted = pd.concat([test, test_sales], axis=1)\nconcatted['error'] = mean_absolute_error(concatted['sales'], concatted['last_sales_ema005'], sample_weight=test_weight)\n\nconcatted.sort_values(by='error')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:30:26.062181Z","iopub.execute_input":"2025-02-17T05:30:26.062511Z","iopub.status.idle":"2025-02-17T05:30:26.144917Z","shell.execute_reply.started":"2025-02-17T05:30:26.062487Z","shell.execute_reply":"2025-02-17T05:30:26.144053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Shap notes below","metadata":{}},{"cell_type":"markdown","source":"I was running into errors when trying to directly apply the shap package functions to the xgboost model, so here's a workaround that I found online.","metadata":{}},{"cell_type":"code","source":"test_dm = DMatrix(test_copy, enable_categorical=True)\nshap_values = xgb.get_booster().predict(test_dm, pred_contribs=True)\nshap.summary_plot(shap_values[:,:-1], test_copy, max_display=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:49:57.323471Z","iopub.execute_input":"2025-02-15T02:49:57.323758Z","iopub.status.idle":"2025-02-15T02:50:13.352936Z","shell.execute_reply.started":"2025-02-15T02:49:57.323737Z","shell.execute_reply":"2025-02-15T02:50:13.352006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I would often look at the shap partial dependence plots for features in the model. This helped uncover some valuable insights for effective feature engineering. For example, one curious thing I noticed was that discount 6 uniquely appears to have a negative correlation with predicted sales. This led me to exclude discount 6 when calculating the max discount. Note that this trend is more apparent before adding max_discount and other discount-based feature engineering to the model).","metadata":{}},{"cell_type":"code","source":"shap.dependence_plot('type_6_discount', shap_values[:,:-1], test_copy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:50:13.354358Z","iopub.execute_input":"2025-02-15T02:50:13.35469Z","iopub.status.idle":"2025-02-15T02:50:14.639275Z","shell.execute_reply.started":"2025-02-15T02:50:13.354656Z","shell.execute_reply":"2025-02-15T02:50:14.638408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sub = test_pred_df.mean(axis=1)\ntest_sub.name = 'sales_hat'\ntest_sub.to_csv('submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:50:34.007466Z","iopub.execute_input":"2025-02-15T02:50:34.007778Z","iopub.status.idle":"2025-02-15T02:50:34.080547Z","shell.execute_reply.started":"2025-02-15T02:50:34.007751Z","shell.execute_reply":"2025-02-15T02:50:34.079906Z"}},"outputs":[],"execution_count":null}]}